{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f29a73e3-579d-40c9-8cb8-13cbe7eb9c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Columns Found: ['year', 'quantity (tonnes)', 'value (rs.in lakhs)']\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\lstm.py:148: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2012 - val_loss: 0.8744\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1865 - val_loss: 0.8328\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1722 - val_loss: 0.7919\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1583 - val_loss: 0.7515\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1448 - val_loss: 0.7116\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1317 - val_loss: 0.6721\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.1191 - val_loss: 0.6330\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1068 - val_loss: 0.5943\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0950 - val_loss: 0.5560\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0836 - val_loss: 0.5181\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0727 - val_loss: 0.4805\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0623 - val_loss: 0.4434\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0524 - val_loss: 0.4067\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0431 - val_loss: 0.3707\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0345 - val_loss: 0.3355\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0267 - val_loss: 0.3011\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0198 - val_loss: 0.2679\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0138 - val_loss: 0.2361\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0089 - val_loss: 0.2060\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0052 - val_loss: 0.1780\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0027 - val_loss: 0.1524\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0015 - val_loss: 0.1296\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0015 - val_loss: 0.1102\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0024 - val_loss: 0.0943\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0041 - val_loss: 0.0820\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0060 - val_loss: 0.0734\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0078 - val_loss: 0.0682\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0090 - val_loss: 0.0659\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0096 - val_loss: 0.0661\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0686\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0087 - val_loss: 0.0729\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0075 - val_loss: 0.0788\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0062 - val_loss: 0.0860\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0048 - val_loss: 0.0940\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0036 - val_loss: 0.1027\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0026 - val_loss: 0.1116\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0019 - val_loss: 0.1205\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0014 - val_loss: 0.1291\n",
      "1/1 [==============================] - 0s 423ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\n",
      "‚úÖ PIPELINE COMPLETED SUCCESSFULLY\n",
      "üìÅ Files saved in: C:\\Users\\NXTWAVE\\Downloads\\Fish Supply Chain Risk Detection\n",
      "üìä Train RMSE: 0.1683337405465287\n",
      "üìä Test RMSE: 0.12250382887861633\n",
      "‚ö†Ô∏è AIS Threshold: 1.0306137800216675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ============================================================\n",
    "# PATHS\n",
    "# ============================================================\n",
    "\n",
    "BASE_DIR = r\"C:\\Users\\NXTWAVE\\Downloads\\Fish Supply Chain Risk Detection\"\n",
    "DATA_PATH = os.path.join(BASE_DIR, \"ExportoffishandfishproductsinTN_0.csv\")\n",
    "\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATA\n",
    "# ============================================================\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "print(\"Columns Found:\", df.columns.tolist())\n",
    "\n",
    "# ============================================================\n",
    "# AUTO FEATURE SELECTION (NUMERIC)\n",
    "# ============================================================\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "\n",
    "if len(numeric_cols) < 2:\n",
    "    raise Exception(\"‚ùå Not enough numeric columns for LSTM modeling\")\n",
    "\n",
    "df = df[numeric_cols].dropna()\n",
    "\n",
    "# ============================================================\n",
    "# SCALING\n",
    "# ============================================================\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# ============================================================\n",
    "# TIME SERIES CREATION\n",
    "# ============================================================\n",
    "\n",
    "def create_sequences(data, window=5):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window):\n",
    "        X.append(data[i:i+window])\n",
    "        y.append(np.mean(data[i+window]))  # risk proxy\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "TIME_STEPS = 5\n",
    "X, y = create_sequences(scaled_data, TIME_STEPS)\n",
    "\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# ============================================================\n",
    "# LSTM MODEL\n",
    "# ============================================================\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, activation=\"tanh\", input_shape=(X.shape[1], X.shape[2])),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mse\"\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# PREDICTION\n",
    "# ============================================================\n",
    "\n",
    "train_pred = model.predict(X_train)\n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "\n",
    "# ============================================================\n",
    "# AIS (Artificial Immune System) ‚Äì ANOMALY RISK SCORING\n",
    "# ============================================================\n",
    "\n",
    "def ais_risk_score(predictions, threshold=None):\n",
    "    mean = np.mean(predictions)\n",
    "    std = np.std(predictions)\n",
    "    if threshold is None:\n",
    "        threshold = mean + 2 * std\n",
    "    risk = np.where(predictions > threshold, 1, predictions / threshold)\n",
    "    return risk.flatten().tolist(), float(threshold)\n",
    "\n",
    "risk_scores, ais_threshold = ais_risk_score(test_pred)\n",
    "\n",
    "# ============================================================\n",
    "# SAVE MODEL & ARTIFACTS\n",
    "# ============================================================\n",
    "\n",
    "# LSTM model\n",
    "model.save(os.path.join(BASE_DIR, \"fish_supply_chain_lstm.h5\"))\n",
    "\n",
    "# Scaler\n",
    "joblib.dump(scaler, os.path.join(BASE_DIR, \"scaler.pkl\"))\n",
    "\n",
    "# ============================================================\n",
    "# SAVE RESULTS CSV\n",
    "# ============================================================\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"actual\": y_test.flatten(),\n",
    "    \"predicted\": test_pred.flatten(),\n",
    "    \"risk_score\": risk_scores\n",
    "})\n",
    "\n",
    "results_df.to_csv(\n",
    "    os.path.join(BASE_DIR, \"risk_results.csv\"),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# SAVE JSON\n",
    "# ============================================================\n",
    "\n",
    "json_data = {\n",
    "    \"train_rmse\": float(train_rmse),\n",
    "    \"test_rmse\": float(test_rmse),\n",
    "    \"ais_threshold\": ais_threshold,\n",
    "    \"total_routes_evaluated\": len(risk_scores),\n",
    "    \"high_risk_routes\": int(sum(r > 0.8 for r in risk_scores))\n",
    "}\n",
    "\n",
    "with open(os.path.join(BASE_DIR, \"risk_summary.json\"), \"w\") as f:\n",
    "    json.dump(json_data, f, indent=4)\n",
    "\n",
    "# ============================================================\n",
    "# SAVE YAML\n",
    "# ============================================================\n",
    "\n",
    "yaml_data = {\n",
    "    \"model\": \"LSTM + AIS\",\n",
    "    \"time_steps\": TIME_STEPS,\n",
    "    \"features_used\": numeric_cols,\n",
    "    \"metrics\": {\n",
    "        \"train_rmse\": float(train_rmse),\n",
    "        \"test_rmse\": float(test_rmse)\n",
    "    },\n",
    "    \"ais\": {\n",
    "        \"threshold\": ais_threshold,\n",
    "        \"description\": \"Artificial Immune System anomaly-based risk detection\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(BASE_DIR, \"config.yaml\"), \"w\") as f:\n",
    "    yaml.dump(yaml_data, f)\n",
    "\n",
    "# ============================================================\n",
    "# SAVE PKL (COMPLETE PIPELINE)\n",
    "# ============================================================\n",
    "\n",
    "pipeline = {\n",
    "    \"model\": model,\n",
    "    \"scaler\": scaler,\n",
    "    \"features\": numeric_cols,\n",
    "    \"time_steps\": TIME_STEPS\n",
    "}\n",
    "\n",
    "joblib.dump(\n",
    "    pipeline,\n",
    "    os.path.join(BASE_DIR, \"full_pipeline.pkl\")\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# FINAL OUTPUT\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n‚úÖ PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "print(\"üìÅ Files saved in:\", BASE_DIR)\n",
    "print(\"üìä Train RMSE:\", train_rmse)\n",
    "print(\"üìä Test RMSE:\", test_rmse)\n",
    "print(\"‚ö†Ô∏è AIS Threshold:\", ais_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d870363-fa29-4351-8efc-057a21a365a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
